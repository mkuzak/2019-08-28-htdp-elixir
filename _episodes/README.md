# Getting started with high-throughput data processing on the Spider platform

**Content**

Vastly increasing data volumes and data complexity imply that researchers require novel solutions to large scale cluster
computing. This workshop aims to provide an introduction and hands-on experience with our new solution to this problem: 
"a new and versatile platform for high-throughput data processing". The objective of the workshop is to distinguish the 
different functionalities of the platform, to provide information on applying for access, and to give participants 
hands-on experience with basic job processing, software distribution and portability, using internal and external 
storage systems, and how to collaborate on data analysis within a project. Participants will acquire first hand experience
with the flexibility, interactivity and interoperability offered by the  platform.

**Target group / Prerequisites**

Anyone who wants to start processing large data volumes (tens to hundreds of terabytes or even more)

  - Familiarity with Linux commands
  
  - Familiarity with batch systems (ssh access, job submission), preferably you have followed Introduction to cluster computing course (cluster-computing)
  
  - Please install Docker to your laptop prior to the course. Install from this [link](https://store.docker.com/search?offering=community&type=edition) (may require creation of a free Docker account)

**Where**: 3.5 in SURF Utrecht

**When**: 13:00-17:00 28 August 2019 (Wednesday)

**Program**:
--------
13:00 - Introduction to the high throughput platform

13:30 - [Login to the platform (ssh key troubleshooting] (https://github.com/maithili-k/singularity-course/blob/master/run-docker.md)

14:00 - Demo of data manager and software manager role

15:00 - Coffee

15:15 - Submitting jobs and using collaboration features

16:15 - More demos of additional features and Wrap up
